{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Example in how to use ChatModel Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from automated_llm_eval.chat_model import ChatModel, Message\n",
    "from automated_llm_eval.utils import ProgressBar, sidethread_event_loop_async_runner\n",
    "\n",
    "# Instantiate wrapper around OpenAI's API\n",
    "model = ChatModel(model=\"gpt-3.5-turbo-1106\")\n",
    "# model = ChatModel(model=\"gpt-4-1106-preview\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can adjust other model settings globally for all API calls\n",
    "model2 = ChatModel(model=\"gpt-3.5-turbo-1106\", temperature=0.5, top_p=0.5, max_tokens=300, seed=42)\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `max_tokens = None` means no max_token limit (this is the default)\n",
    "model2 = ChatModel(model=\"gpt-3.5-turbo-1106\", temperature=0.5, top_p=0.5, max_tokens=None, seed=42)\n",
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Making API calls using synchronous (blocking) client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make API call, get response message.\n",
    "# Note: `output_format = \"simple\"`\n",
    "response_message = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=\"simple\",\n",
    ")\n",
    "print(response_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make API call, get original ChatCompletion object.\n",
    "# Note: `output_format = None`\n",
    "response = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=None,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make API call, get response packaged with input + metadata.\n",
    "# Note: `output_format = \"bundle\"`\n",
    "bundle = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=\"bundle\",\n",
    ")\n",
    "print(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make API call, get MessageBundle as a dict.\n",
    "# Note: `output_format = \"bundle_dict\"`\n",
    "bundle_dict = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=\"bundle_dict\",\n",
    ")\n",
    "print(bundle_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message bundle dict can be converted into pandas Series easily\n",
    "s = pd.Series(bundle_dict)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple message bundle dicts can be converted into pandas DataFrame\n",
    "# NOTE: if an API call fails, then `None` will be returned. `None` items cannot\n",
    "# be directly converted into pd.DataFrame\n",
    "responses = []\n",
    "with ProgressBar() as p:\n",
    "    for _ in p.track(range(5)):\n",
    "        response = model.create_chat_completion(\n",
    "            system_message=\"You are a joke telling machine.\",\n",
    "            user_message=\"Tell me something about apples.\",\n",
    "            output_format=\"bundle_dict\",\n",
    "            temperature=0.4,\n",
    "            seed=None,\n",
    "        )\n",
    "        responses += [response]\n",
    "\n",
    "df = pd.DataFrame(responses)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If an API call fails, this method will automatically retry and make another API call.\n",
    "# By default it will retry 5 times.  We can change this value to 2.\n",
    "bundle_dict = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=\"bundle_dict\",\n",
    "    num_retries=2,\n",
    ")\n",
    "print(bundle_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `create_chat_completion` method is syntactic sugar for `chat_completion`.\n",
    "# It simply formats the message for us.\n",
    "system_message = \"You are a joke telling machine.\"\n",
    "user_message = \"Tell me something about apples.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "\n",
    "bundle_dict = model.chat_completion(\n",
    "    messages=messages,\n",
    "    output_format=\"bundle_dict\",\n",
    "    num_retries=2,\n",
    ")\n",
    "print(bundle_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Making API calls using asynchronous (non-blocking) client\n",
    "\n",
    " This enables concurrent API calls.  We can control the max concurrency.\n",
    "\n",
    " Async uses the asyncio paradigm.  We need to run an asyncio event loop to\n",
    " use these functions.\n",
    " NOTE: a jupyter notebook has an asyncio event loop running by default,\n",
    " but you need to create your own asyncio event loop in a python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a joke telling machine.\"\n",
    "user_message = \"Tell me something about apples.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "\n",
    "response = await model.async_chat_completion(messages=messages, num_retries=1)  # noqa: F704:\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate Messages x 5 times so that we can make 5 API calls\n",
    "messages_list = [messages] * 5\n",
    "messages_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Async Chat Completions, limit to 2 concurrent API calls at any given time\n",
    "responses_list = await model.async_chat_completions(  # noqa: F704\n",
    "    messages_list=messages_list,\n",
    "    num_concurrent=2,\n",
    "    num_retries=1,\n",
    "    output_format=\"bundle_dict\",\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(responses_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Example of using `Message` and `validation_callback`\n",
    "\n",
    " The `Message` wrapper allows packaging arbitrary user-defined metadata along with each message\n",
    " which is a good place to put labels, notes, etc.\n",
    "\n",
    " The `validation_callback` argument enables the user to define\n",
    " specific logic to validate the response from each API call to OpenAI\n",
    " for each message.  Passed into the callback function is the original\n",
    " `messages` and the `response`.  If the `messages` is a `Message` object,\n",
    " this will be returned in `validation_callback` for access to all metadata.\n",
    " `response` is the LLM response after being parsed and formated as specified\n",
    " in `output_format`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a joke telling machine.\"\n",
    "user_message = \"Tell me something about apples.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "m = Message(messages=messages, metadata={\"a\": 1})\n",
    "\n",
    "\n",
    "def validation_callback_fn(messages, response) -> bool:\n",
    "    print(f\"In Callback. Messages: {messages}\")\n",
    "    print(f\"In Callback. Response: {response}\")\n",
    "    print(\"\\n\")\n",
    "    metadata = messages.metadata\n",
    "    if \"a\" in metadata:\n",
    "        return metadata[\"a\"] == 1\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Instantiate wrapper around OpenAI's API\n",
    "model = ChatModel(model=\"gpt-3.5-turbo-1106\")\n",
    "# Make ChatCompletion with...\n",
    "# - using Message wrapper and include metadata (ChatModel automatically unpacks Message.messages)\n",
    "# - parse raw OpenAI response into \"simple\" string format\n",
    "# - then call the `validation_callback_fn` that we defined.  ChatModel always passes in\n",
    "#   original messages input and parsed response as the 1st and 2nd arguments.  The\n",
    "#   `validation_callback_fn` can contain any logic, but ultimately needs to return `True` vs `False`\n",
    "#   to accept or reject the response.  If the response is rejected, ChatModel automatically retries.\n",
    "# - allow up to 1 retry.  If still fails/rejected after 1 retry, then will return `None`.\n",
    "response = model.chat_completion(\n",
    "    m,\n",
    "    output_format=\"bundle_dict\",\n",
    "    validation_callback=validation_callback_fn,\n",
    "    num_retries=1,\n",
    ")\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple concurrent async chat completions using Message\n",
    "# NOTE: we make the 3rd Message with different metadata.  This should cause\n",
    "# the `validation_callback_fn` to reject the response for only the 3rd Message in list\n",
    "# and retry only the 3rd Message.\n",
    "msg_list = [m] * 2 + [Message(messages=messages, metadata={\"b\": 2})]\n",
    "msg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Async Chat Completions, limit to 2 concurrent API calls at any given time & 1 retry\n",
    "responses_list = await model.async_chat_completions(  # noqa: F704\n",
    "    messages_list=msg_list,\n",
    "    num_concurrent=2,\n",
    "    num_retries=1,\n",
    "    validation_callback=validation_callback_fn,\n",
    "    output_format=\"bundle_dict\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine responses.\n",
    "# - We should get valid responses for the first 2 responses.\n",
    "# - The 3rd response should always be `None` because the metadata cannot pass at\n",
    "#   `validation_callback_fn`\n",
    "responses_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Calling Async function from Sync code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatModel(model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "system_message = \"You are a joke telling machine.\"\n",
    "user_message = \"Tell me something about apples.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "m = Message(messages=messages, metadata={\"a\": 1})\n",
    "msg_list = [m] * 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up until now, we have used `await` to call async functions and wait for their completion.\n",
    "# However, `await` this can only be used within async functions.\n",
    "# we are not allowed to call `await` from a function not defined with `async def`\n",
    "responses = await model.async_chat_completions(\n",
    "    messages_list=msg_list, num_concurrent=2, output_format=\"bundle\"\n",
    ")\n",
    "responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have created a helper function to address this issue.\n",
    "#\n",
    "# Call async method from sync function without using `await` keyword.\n",
    "# This involves creating an event loop on another thread, then\n",
    "# waiting for result on main thread and shutting down the event loop on other thread.\n",
    "\n",
    "result = sidethread_event_loop_async_runner(\n",
    "    async_function=model.async_chat_completions(\n",
    "        messages_list=msg_list, num_concurrent=2, output_format=\"bundle\"\n",
    "    )\n",
    ")\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
