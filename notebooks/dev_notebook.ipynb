{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Example in how to use ChatModel Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatModel(sync_client=<openai.OpenAI object at 0x11ca88d90>, async_client=<openai.AsyncOpenAI object at 0x11caac1d0>, model='gpt-3.5-turbo-1106', temperature=0.9, top_p=0.9, max_tokens=None, n=1, seed=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from automated_llm_eval.chat_model import ChatModel, Message\n",
    "from automated_llm_eval.utils import ProgressBar, sidethread_event_loop_async_runner\n",
    "\n",
    "# Instantiate wrapper around OpenAI's API\n",
    "model = ChatModel(model=\"gpt-3.5-turbo-1106\")\n",
    "# model = ChatModel(model=\"gpt-4-1106-preview\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatModel(sync_client=<openai.OpenAI object at 0x11f488ed0>, async_client=<openai.AsyncOpenAI object at 0x11f4ac390>, model='gpt-3.5-turbo-1106', temperature=0.5, top_p=0.5, max_tokens=300, n=1, seed=42)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can adjust other model settings globally for all API calls\n",
    "model2 = ChatModel(model=\"gpt-3.5-turbo-1106\", temperature=0.5, top_p=0.5, max_tokens=300, seed=42)\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatModel(sync_client=<openai.OpenAI object at 0x11f488ed0>, async_client=<openai.AsyncOpenAI object at 0x11f4ac390>, model='gpt-3.5-turbo-1106', temperature=0.5, top_p=0.5, max_tokens=None, n=1, seed=42)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `max_tokens = None` means no max_token limit (this is the default)\n",
    "model2 = ChatModel(model=\"gpt-3.5-turbo-1106\", temperature=0.5, top_p=0.5, max_tokens=None, seed=42)\n",
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Making API calls using synchronous (blocking) client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Did you hear about the apple who went to the doctor? It wasn't feeling well, so the doctor said, \"You're just a little too core-ny!\"\n"
     ]
    }
   ],
   "source": [
    "# Make API call, get response message.\n",
    "# Note: `output_format = \"simple\"`\n",
    "response_message = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=\"simple\",\n",
    ")\n",
    "print(response_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8LHFVTIKmcNyrmyoBng465aJUzPmJ', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Sure! Did you hear about the apple that went to a comedy club? It was a real \"core\" comedian!', role='assistant', function_call=None, tool_calls=None))], created=1700081885, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage=CompletionUsage(completion_tokens=24, prompt_tokens=24, total_tokens=48))\n"
     ]
    }
   ],
   "source": [
    "# Make API call, get original ChatCompletion object.\n",
    "# Note: `output_format = None`\n",
    "response = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=None,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bundle(id='chatcmpl-8LHFWxY85RSdhc9Jur0LEwdcuhVIP', system_message='You are a joke telling machine.', user_message='Tell me something about apples.', response_message='Sure! Did you hear about the apple that joined a band? It was a real \"fruit\" musician!', created_time=1700081886, model='gpt-3.5-turbo-1106', total_tokens=46, prompt_tokens=24, completion_tokens=22, seed=None, temperature=0.9, top_p=0.9, max_tokens=None)\n"
     ]
    }
   ],
   "source": [
    "# Make API call, get response packaged with input + metadata.\n",
    "# Note: `output_format = \"bundle\"`\n",
    "bundle = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=\"bundle\",\n",
    ")\n",
    "print(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8LHFX3WIvSFJwf6tdnjoQ9H4Ix6XO', 'system_message': 'You are a joke telling machine.', 'user_message': 'Tell me something about apples.', 'response_message': \"Why did the apple go to the doctor? Because it wasn't peeling well!\", 'created_time': 1700081887, 'model': 'gpt-3.5-turbo-1106', 'total_tokens': 41, 'prompt_tokens': 24, 'completion_tokens': 17, 'seed': None, 'temperature': 0.9, 'top_p': 0.9, 'max_tokens': None}\n"
     ]
    }
   ],
   "source": [
    "# Make API call, get MessageBundle as a dict.\n",
    "# Note: `output_format = \"bundle_dict\"`\n",
    "bundle_dict = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=\"bundle_dict\",\n",
    ")\n",
    "print(bundle_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                              chatcmpl-8LHFX3WIvSFJwf6tdnjoQ9H4Ix6XO\n",
       "system_message                         You are a joke telling machine.\n",
       "user_message                           Tell me something about apples.\n",
       "response_message     Why did the apple go to the doctor? Because it...\n",
       "created_time                                                1700081887\n",
       "model                                               gpt-3.5-turbo-1106\n",
       "total_tokens                                                        41\n",
       "prompt_tokens                                                       24\n",
       "completion_tokens                                                   17\n",
       "seed                                                              None\n",
       "temperature                                                        0.9\n",
       "top_p                                                              0.9\n",
       "max_tokens                                                        None\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Message bundle dict can be converted into pandas Series easily\n",
    "s = pd.Series(bundle_dict)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working... 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5/5 • 0:00:13 • 0:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>system_message</th>\n",
       "      <th>user_message</th>\n",
       "      <th>response_message</th>\n",
       "      <th>created_time</th>\n",
       "      <th>model</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>seed</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>max_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chatcmpl-8LHTZwe8mtAmyl3T06VXpBGnexLfT</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Sure! Did you hear about the apple who won the...</td>\n",
       "      <td>1700082757</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>48</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chatcmpl-8LHTbLMzUcU92YIXRXcxyV1Qrg9n6</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Why did the apple go to the doctor? Because it...</td>\n",
       "      <td>1700082759</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>41</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chatcmpl-8LHTd1NGz4ddsbrOLAeivECQ6BfDV</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Why did the apple go to the doctor? Because it...</td>\n",
       "      <td>1700082761</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>41</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chatcmpl-8LHTln8gLfOQoWeE2BVcbpsq7TMJm</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Sure! Did you hear about the apple who went to...</td>\n",
       "      <td>1700082769</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>59</td>\n",
       "      <td>24</td>\n",
       "      <td>35</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chatcmpl-8LHTndPlf4u989oUvyPp6y9CPPaa9</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Sure! Did you hear about the apple that went t...</td>\n",
       "      <td>1700082771</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>45</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id                   system_message  \\\n",
       "0  chatcmpl-8LHTZwe8mtAmyl3T06VXpBGnexLfT  You are a joke telling machine.   \n",
       "1  chatcmpl-8LHTbLMzUcU92YIXRXcxyV1Qrg9n6  You are a joke telling machine.   \n",
       "2  chatcmpl-8LHTd1NGz4ddsbrOLAeivECQ6BfDV  You are a joke telling machine.   \n",
       "3  chatcmpl-8LHTln8gLfOQoWeE2BVcbpsq7TMJm  You are a joke telling machine.   \n",
       "4  chatcmpl-8LHTndPlf4u989oUvyPp6y9CPPaa9  You are a joke telling machine.   \n",
       "\n",
       "                      user_message  \\\n",
       "0  Tell me something about apples.   \n",
       "1  Tell me something about apples.   \n",
       "2  Tell me something about apples.   \n",
       "3  Tell me something about apples.   \n",
       "4  Tell me something about apples.   \n",
       "\n",
       "                                    response_message  created_time  \\\n",
       "0  Sure! Did you hear about the apple who won the...    1700082757   \n",
       "1  Why did the apple go to the doctor? Because it...    1700082759   \n",
       "2  Why did the apple go to the doctor? Because it...    1700082761   \n",
       "3  Sure! Did you hear about the apple who went to...    1700082769   \n",
       "4  Sure! Did you hear about the apple that went t...    1700082771   \n",
       "\n",
       "                model  total_tokens  prompt_tokens  completion_tokens  seed  \\\n",
       "0  gpt-3.5-turbo-1106            48             24                 24  None   \n",
       "1  gpt-3.5-turbo-1106            41             24                 17  None   \n",
       "2  gpt-3.5-turbo-1106            41             24                 17  None   \n",
       "3  gpt-3.5-turbo-1106            59             24                 35  None   \n",
       "4  gpt-3.5-turbo-1106            45             24                 21  None   \n",
       "\n",
       "   temperature  top_p max_tokens  \n",
       "0          0.4    0.9       None  \n",
       "1          0.4    0.9       None  \n",
       "2          0.4    0.9       None  \n",
       "3          0.4    0.9       None  \n",
       "4          0.4    0.9       None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiple message bundle dicts can be converted into pandas DataFrame\n",
    "# NOTE: if an API call fails, then `None` will be returned. `None` items cannot\n",
    "# be directly converted into pd.DataFrame\n",
    "responses = []\n",
    "with ProgressBar() as p:\n",
    "    for _ in p.track(range(5)):\n",
    "        response = model.create_chat_completion(\n",
    "            system_message=\"You are a joke telling machine.\",\n",
    "            user_message=\"Tell me something about apples.\",\n",
    "            output_format=\"bundle_dict\",\n",
    "            temperature=0.4,\n",
    "            seed=None,\n",
    "        )\n",
    "        responses += [response]\n",
    "\n",
    "df = pd.DataFrame(responses)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8LHFe3TwPLPVF5apK6rbRGC34iLR9', 'system_message': 'You are a joke telling machine.', 'user_message': 'Tell me something about apples.', 'response_message': \"Why did the apple go to the doctor? Because it wasn't peeling well!\", 'created_time': 1700081894, 'model': 'gpt-3.5-turbo-1106', 'total_tokens': 41, 'prompt_tokens': 24, 'completion_tokens': 17, 'seed': None, 'temperature': 0.9, 'top_p': 0.9, 'max_tokens': None}\n"
     ]
    }
   ],
   "source": [
    "# If an API call fails, this method will automatically retry and make another API call.\n",
    "# By default it will retry 5 times.  We can change this value to 2.\n",
    "bundle_dict = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=\"bundle_dict\",\n",
    "    num_retries=2,\n",
    ")\n",
    "print(bundle_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8LHFfL1BpS9A4Z2dk6EIpQHnIXVP4', 'system_message': 'You are a joke telling machine.', 'user_message': 'Tell me something about apples.', 'response_message': \"Why did the apple go to the doctor? Because it wasn't peeling well!\", 'created_time': 1700081895, 'model': 'gpt-3.5-turbo-1106', 'total_tokens': 41, 'prompt_tokens': 24, 'completion_tokens': 17, 'seed': None, 'temperature': 0.9, 'top_p': 0.9, 'max_tokens': None}\n"
     ]
    }
   ],
   "source": [
    "# The `create_chat_completion` method is syntactic sugar for `chat_completion`.\n",
    "# It simply formats the message for us.\n",
    "system_message = \"You are a joke telling machine.\"\n",
    "user_message = \"Tell me something about apples.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "\n",
    "bundle_dict = model.chat_completion(\n",
    "    messages=messages,\n",
    "    output_format=\"bundle_dict\",\n",
    "    num_retries=2,\n",
    ")\n",
    "print(bundle_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Making API calls using asynchronous (non-blocking) client\n",
    "\n",
    " This enables concurrent API calls.  We can control the max concurrency.\n",
    "\n",
    " Async uses the asyncio paradigm.  We need to run an asyncio event loop to\n",
    " use these functions.\n",
    " NOTE: a jupyter notebook has an asyncio event loop running by default,\n",
    " but you need to create your own asyncio event loop in a python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8LHFiXgCVD8lu3ncjwKD0ULlgi5Rx', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Sure! Did you hear about the apple who went to the doctor? The doctor said, \"You\\'re not looking so hot, you should probably see a cider-ist!\"', role='assistant', function_call=None, tool_calls=None))], created=1700081898, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage=CompletionUsage(completion_tokens=35, prompt_tokens=24, total_tokens=59))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"You are a joke telling machine.\"\n",
    "user_message = \"Tell me something about apples.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "\n",
    "response = await model.async_chat_completion(messages=messages, num_retries=1)  # noqa: F704:\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'system', 'content': 'You are a joke telling machine.'},\n",
       "  {'role': 'user', 'content': 'Tell me something about apples.'}],\n",
       " [{'role': 'system', 'content': 'You are a joke telling machine.'},\n",
       "  {'role': 'user', 'content': 'Tell me something about apples.'}],\n",
       " [{'role': 'system', 'content': 'You are a joke telling machine.'},\n",
       "  {'role': 'user', 'content': 'Tell me something about apples.'}],\n",
       " [{'role': 'system', 'content': 'You are a joke telling machine.'},\n",
       "  {'role': 'user', 'content': 'Tell me something about apples.'}],\n",
       " [{'role': 'system', 'content': 'You are a joke telling machine.'},\n",
       "  {'role': 'user', 'content': 'Tell me something about apples.'}]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate Messages x 5 times so that we can make 5 API calls\n",
    "messages_list = [messages] * 5\n",
    "messages_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67669804ad9a4e9b9b96c76be94e0e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>system_message</th>\n",
       "      <th>user_message</th>\n",
       "      <th>response_message</th>\n",
       "      <th>created_time</th>\n",
       "      <th>model</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>seed</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>max_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chatcmpl-8LHFkpQFlOy3Mm1kRU5ztKUBrGSvn</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Why did the apple stop in the middle of the ro...</td>\n",
       "      <td>1700081900</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>43</td>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "      <td>None</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chatcmpl-8LHFkcxmbRkorvjQ0rnqMdaA7jekx</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Why did the apple go to the doctor? Because it...</td>\n",
       "      <td>1700081900</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>41</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chatcmpl-8LHFl5eCUj6urcXpbH1QdD3p2zZ7U</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Sure! Did you hear about the apple that went o...</td>\n",
       "      <td>1700081901</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>52</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "      <td>None</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chatcmpl-8LHFm4YFHXxzZS8kGPvYPQDqahpXM</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Sure! Did you hear about the apple that won th...</td>\n",
       "      <td>1700081902</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>51</td>\n",
       "      <td>24</td>\n",
       "      <td>27</td>\n",
       "      <td>None</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chatcmpl-8LHFnBoOMlN4WsVJfctUsrjUJMRYC</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Why did the apple break up with the banana?\\n\\...</td>\n",
       "      <td>1700081903</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>43</td>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "      <td>None</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id                   system_message  \\\n",
       "0  chatcmpl-8LHFkpQFlOy3Mm1kRU5ztKUBrGSvn  You are a joke telling machine.   \n",
       "1  chatcmpl-8LHFkcxmbRkorvjQ0rnqMdaA7jekx  You are a joke telling machine.   \n",
       "2  chatcmpl-8LHFl5eCUj6urcXpbH1QdD3p2zZ7U  You are a joke telling machine.   \n",
       "3  chatcmpl-8LHFm4YFHXxzZS8kGPvYPQDqahpXM  You are a joke telling machine.   \n",
       "4  chatcmpl-8LHFnBoOMlN4WsVJfctUsrjUJMRYC  You are a joke telling machine.   \n",
       "\n",
       "                      user_message  \\\n",
       "0  Tell me something about apples.   \n",
       "1  Tell me something about apples.   \n",
       "2  Tell me something about apples.   \n",
       "3  Tell me something about apples.   \n",
       "4  Tell me something about apples.   \n",
       "\n",
       "                                    response_message  created_time  \\\n",
       "0  Why did the apple stop in the middle of the ro...    1700081900   \n",
       "1  Why did the apple go to the doctor? Because it...    1700081900   \n",
       "2  Sure! Did you hear about the apple that went o...    1700081901   \n",
       "3  Sure! Did you hear about the apple that won th...    1700081902   \n",
       "4  Why did the apple break up with the banana?\\n\\...    1700081903   \n",
       "\n",
       "                model  total_tokens  prompt_tokens  completion_tokens  seed  \\\n",
       "0  gpt-3.5-turbo-1106            43             24                 19  None   \n",
       "1  gpt-3.5-turbo-1106            41             24                 17  None   \n",
       "2  gpt-3.5-turbo-1106            52             24                 28  None   \n",
       "3  gpt-3.5-turbo-1106            51             24                 27  None   \n",
       "4  gpt-3.5-turbo-1106            43             24                 19  None   \n",
       "\n",
       "   temperature  top_p max_tokens  \n",
       "0          0.9    0.9       None  \n",
       "1          0.9    0.9       None  \n",
       "2          0.9    0.9       None  \n",
       "3          0.9    0.9       None  \n",
       "4          0.9    0.9       None  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Async Chat Completions, limit to 2 concurrent API calls at any given time\n",
    "responses_list = await model.async_chat_completions(  # noqa: F704\n",
    "    messages_list=messages_list,\n",
    "    num_concurrent=2,\n",
    "    num_retries=1,\n",
    "    output_format=\"bundle_dict\",\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(responses_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Example of using `Message` and `validation_callback`\n",
    "\n",
    " The `Message` wrapper allows packaging arbitrary user-defined metadata along with each message\n",
    " which is a good place to put labels, notes, etc.\n",
    "\n",
    " The `validation_callback` argument enables the user to define\n",
    " specific logic to validate the response from each API call to OpenAI\n",
    " for each message.  Passed into the callback function is the original\n",
    " `messages` and the `response`.  If the `messages` is a `Message` object,\n",
    " this will be returned in `validation_callback` for access to all metadata.\n",
    " `response` is the LLM response after being parsed and formated as specified\n",
    " in `output_format`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role': 'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1})\n",
      "In Callback. Response: Why did the apple stop in the middle of the road?\n",
      "\n",
      "Because it ran out of juice!\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Why did the apple stop in the middle of the road?\\n\\nBecause it ran out of juice!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"You are a joke telling machine.\"\n",
    "user_message = \"Tell me something about apples.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "m = Message(messages=messages, metadata={\"a\": 1})\n",
    "\n",
    "\n",
    "def validation_callback_fn(messages, response) -> bool:\n",
    "    print(f\"In Callback. Messages: {messages}\")\n",
    "    print(f\"In Callback. Response: {response}\")\n",
    "    print(\"\\n\")\n",
    "    metadata = messages.metadata\n",
    "    if \"a\" in metadata:\n",
    "        return metadata[\"a\"] == 1\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Instantiate wrapper around OpenAI's API\n",
    "model = ChatModel(model=\"gpt-3.5-turbo-1106\")\n",
    "# Make ChatCompletion with...\n",
    "# - using Message wrapper and include metadata (ChatModel automatically unpacks Message.messages)\n",
    "# - parse raw OpenAI response into \"simple\" string format\n",
    "# - then call the `validation_callback_fn` that we defined.  ChatModel always passes in\n",
    "#   original messages input and parsed response as the 1st and 2nd arguments.  The\n",
    "#   `validation_callback_fn` can contain any logic, but ultimately needs to return `True` vs `False`\n",
    "#   to accept or reject the response.  If the response is rejected, ChatModel automatically retries.\n",
    "# - allow up to 1 retry.  If still fails/rejected after 1 retry, then will return `None`.\n",
    "response = model.chat_completion(\n",
    "    m,\n",
    "    output_format=\"simple\",\n",
    "    validation_callback=validation_callback_fn,\n",
    "    num_retries=1,\n",
    ")\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role': 'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1}),\n",
       " Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role': 'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1}),\n",
       " Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role': 'user', 'content': 'Tell me something about apples.'}], metadata={'b': 2})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiple concurrent async chat completions using Message\n",
    "# NOTE: we make the 3rd Message with different metadata.  This should cause\n",
    "# the `validation_callback_fn` to reject the response for only the 3rd Message in list\n",
    "# and retry only the 3rd Message.\n",
    "msg_list = [m] * 2 + [Message(messages=messages, metadata={\"b\": 2})]\n",
    "msg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9584a235e14604ab75d5d4f426c231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role':\n",
       "'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1})\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role':\n",
       "'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1})\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Callback. Response: Why did the apple go to the doctor? Because it wasn't peeling well!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Callback. Response: Why did the apple go to the doctor? Because it wasn't peeling well!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role':\n",
       "'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1})\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role':\n",
       "'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1})\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Callback. Response: Why did the apple stop in the middle of the road? Because it ran out of juice!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Callback. Response: Why did the apple stop in the middle of the road? Because it ran out of juice!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role':\n",
       "'user', 'content': 'Tell me something about apples.'}], metadata={'b': 2})\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role':\n",
       "'user', 'content': 'Tell me something about apples.'}], metadata={'b': 2})\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Callback. Response: Why did the apple go to the doctor? Because it wasn't peeling well!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Callback. Response: Why did the apple go to the doctor? Because it wasn't peeling well!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use Async Chat Completions, limit to 2 concurrent API calls at any given time & 1 retry\n",
    "responses_list = await model.async_chat_completions(  # noqa: F704\n",
    "    messages_list=msg_list,\n",
    "    num_concurrent=2,\n",
    "    num_retries=1,\n",
    "    validation_callback=validation_callback_fn,\n",
    "    output_format=\"simple\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Why did the apple go to the doctor? Because it wasn't peeling well!\",\n",
       " 'Why did the apple stop in the middle of the road? Because it ran out of juice!',\n",
       " None]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine responses.\n",
    "# - We should get valid responses for the first 2 responses.\n",
    "# - The 3rd response should always be `None` because the metadata cannot pass at\n",
    "#   `validation_callback_fn`\n",
    "responses_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Calling Async function from Sync code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatModel(model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "system_message = \"You are a joke telling machine.\"\n",
    "user_message = \"Tell me something about apples.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "m = Message(messages=messages, metadata={\"a\": 1})\n",
    "msg_list = [m] * 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875366fc4688437789af768907e1c729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Bundle(id='chatcmpl-8LHG0xLSG28e74jLvsIE7t7R9b1mm', system_message='You are a joke telling machine.', user_message='Tell me something about apples.', response_message='Why did the apple stop in the middle of the road? Because it ran out of juice!', created_time=1700081916, model='gpt-3.5-turbo-1106', total_tokens=43, prompt_tokens=24, completion_tokens=19, seed=None, temperature=0.9, top_p=0.9, max_tokens=None),\n",
       " Bundle(id='chatcmpl-8LHG02RSYvxNGW0IfApWHnwaJv29n', system_message='You are a joke telling machine.', user_message='Tell me something about apples.', response_message='Why did the apple go to therapy? Because it had a core issue!', created_time=1700081916, model='gpt-3.5-turbo-1106', total_tokens=39, prompt_tokens=24, completion_tokens=15, seed=None, temperature=0.9, top_p=0.9, max_tokens=None),\n",
       " Bundle(id='chatcmpl-8LHG1HlqTAMrReI3wB66y2CvKZLRY', system_message='You are a joke telling machine.', user_message='Tell me something about apples.', response_message='Why did the apple go to therapy? Because it had too many cores issues!', created_time=1700081917, model='gpt-3.5-turbo-1106', total_tokens=40, prompt_tokens=24, completion_tokens=16, seed=None, temperature=0.9, top_p=0.9, max_tokens=None)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Up until now, we have used `await` to call async functions and wait for their completion.\n",
    "# However, `await` this can only be used within async functions.\n",
    "# we are not allowed to call `await` from a function not defined with `async def`\n",
    "responses = await model.async_chat_completions(\n",
    "    messages_list=msg_list, num_concurrent=2, output_format=\"bundle\"\n",
    ")\n",
    "responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a161d5fbad5b407e865deab6bd6f8bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Bundle(id='chatcmpl-8LHG5m7piUbUsBWqFS2nwuaQuOhVu', system_message='You are a joke telling machine.', user_message='Tell me something about apples.', response_message=\"Why did the apple break up with the orange? Because it couldn't find a core connection!\", created_time=1700081921, model='gpt-3.5-turbo-1106', total_tokens=43, prompt_tokens=24, completion_tokens=19, seed=None, temperature=0.9, top_p=0.9, max_tokens=None),\n",
       " Bundle(id='chatcmpl-8LHG5cvTHsyd6QBL33g6hTKqtv6SP', system_message='You are a joke telling machine.', user_message='Tell me something about apples.', response_message='Sure! Did you hear about the apple that went on a date with a banana? It was a fruit match made in heaven!', created_time=1700081921, model='gpt-3.5-turbo-1106', total_tokens=50, prompt_tokens=24, completion_tokens=26, seed=None, temperature=0.9, top_p=0.9, max_tokens=None),\n",
       " Bundle(id='chatcmpl-8LHG6HJ0Zs3AfIDArMRRoyzp1Cq6X', system_message='You are a joke telling machine.', user_message='Tell me something about apples.', response_message='Why did the apple go to school?\\n\\nTo get a little bit of \"core\" education!', created_time=1700081922, model='gpt-3.5-turbo-1106', total_tokens=43, prompt_tokens=24, completion_tokens=19, seed=None, temperature=0.9, top_p=0.9, max_tokens=None)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have created a helper function to address this issue.\n",
    "#\n",
    "# Call async method from sync function without using `await` keyword.\n",
    "# This involves creating an event loop on another thread, then\n",
    "# waiting for result on main thread and shutting down the event loop on other thread.\n",
    "\n",
    "result = sidethread_event_loop_async_runner(\n",
    "    async_function=model.async_chat_completions(\n",
    "        messages_list=msg_list, num_concurrent=2, output_format=\"bundle\"\n",
    "    )\n",
    ")\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
