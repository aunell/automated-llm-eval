{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Example in how to use ChatModel Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatModel(sync_client=<openai.OpenAI object at 0x11ca1ce90>, async_client=<openai.AsyncOpenAI object at 0x11ca40190>, model='gpt-3.5-turbo-1106', temperature=0.9, top_p=0.9, max_tokens=None, n=1, seed=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from automated_llm_eval.chat_model import ChatModel, Message\n",
    "from automated_llm_eval.utils import ProgressBar\n",
    "\n",
    "# Instantiate wrapper around OpenAI's API\n",
    "model = ChatModel(model=\"gpt-3.5-turbo-1106\")\n",
    "# model = ChatModel(model=\"gpt-4-1106-preview\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatModel(sync_client=<openai.OpenAI object at 0x11f489150>, async_client=<openai.AsyncOpenAI object at 0x11f4ac490>, model='gpt-3.5-turbo-1106', temperature=0.5, top_p=0.5, max_tokens=300, n=1, seed=42)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can adjust other model settings globally for all API calls\n",
    "model2 = ChatModel(model=\"gpt-3.5-turbo-1106\", temperature=0.5, top_p=0.5, max_tokens=300, seed=42)\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatModel(sync_client=<openai.OpenAI object at 0x11f489150>, async_client=<openai.AsyncOpenAI object at 0x11f4ac490>, model='gpt-3.5-turbo-1106', temperature=0.5, top_p=0.5, max_tokens=None, n=1, seed=42)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `max_tokens = None` means no max_token limit (this is the default)\n",
    "model2 = ChatModel(model=\"gpt-3.5-turbo-1106\", temperature=0.5, top_p=0.5, max_tokens=None, seed=42)\n",
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Making API calls using synchronous (blocking) client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Did you hear about the apple that went to school? It got voted \"most popular\" in the lunchbox!\n"
     ]
    }
   ],
   "source": [
    "# Make API call, get response message.\n",
    "# Note: `output_format = \"simple\"`\n",
    "response_message = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=\"simple\",\n",
    ")\n",
    "print(response_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8L3gRrlWtthxN4656Dhys0OtR1uit', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Sure! Did you hear about the apple who joined a rock band? He was a real \"core\" musician!', role='assistant', function_call=None, tool_calls=None))], created=1700029739, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage=CompletionUsage(completion_tokens=23, prompt_tokens=24, total_tokens=47))\n"
     ]
    }
   ],
   "source": [
    "# Make API call, get original ChatCompletion object.\n",
    "# Note: `output_format = None`\n",
    "response = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=None,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bundle(id='chatcmpl-8L3gTSri0AarIL6H5H3uAx3u3S4WX', system_message='You are a joke telling machine.', user_message='Tell me something about apples.', response_message=\"Why did the apple go to the doctor?\\n\\nBecause it wasn't peeling well!\", created_time=1700029741, model='gpt-3.5-turbo-1106', total_tokens=41, prompt_tokens=24, completion_tokens=17, seed=None, temperature=0.9, top_p=0.9, max_tokens=None)\n"
     ]
    }
   ],
   "source": [
    "# Make API call, get response packaged with input + metadata.\n",
    "# Note: `output_format = \"bundle\"`\n",
    "bundle = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=\"bundle\",\n",
    ")\n",
    "print(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8L3gV11hvyxjjo3wJeKAEwLrukE96', 'system_message': 'You are a joke telling machine.', 'user_message': 'Tell me something about apples.', 'response_message': 'Sure! Did you know that apples are great at making jokes? They always know how to \"core\" up a good laugh!', 'created_time': 1700029743, 'model': 'gpt-3.5-turbo-1106', 'total_tokens': 50, 'prompt_tokens': 24, 'completion_tokens': 26, 'seed': None, 'temperature': 0.9, 'top_p': 0.9, 'max_tokens': None}\n"
     ]
    }
   ],
   "source": [
    "# Make API call, get MessageBundle as a dict.\n",
    "# Note: `output_format = \"bundle_dict\"`\n",
    "bundle_dict = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=\"bundle_dict\",\n",
    ")\n",
    "print(bundle_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                              chatcmpl-8L3gV11hvyxjjo3wJeKAEwLrukE96\n",
       "system_message                         You are a joke telling machine.\n",
       "user_message                           Tell me something about apples.\n",
       "response_message     Sure! Did you know that apples are great at ma...\n",
       "created_time                                                1700029743\n",
       "model                                               gpt-3.5-turbo-1106\n",
       "total_tokens                                                        50\n",
       "prompt_tokens                                                       24\n",
       "completion_tokens                                                   26\n",
       "seed                                                              None\n",
       "temperature                                                        0.9\n",
       "top_p                                                              0.9\n",
       "max_tokens                                                        None\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Message bundle dict can be converted into pandas Series easily\n",
    "s = pd.Series(bundle_dict)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6ea6a87b5f4603b4372d1f2d6e5b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>system_message</th>\n",
       "      <th>user_message</th>\n",
       "      <th>response_message</th>\n",
       "      <th>created_time</th>\n",
       "      <th>model</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>seed</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>max_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chatcmpl-8L3gbJgmQkv7MAp1k9g7PWMFVf2fv</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Why did the apple go to the doctor? Because it...</td>\n",
       "      <td>1700029749</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>41</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chatcmpl-8L3gbjIEI67yx86MuT8AxJUcvTUZl</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Why did the apple go to the doctor? Because it...</td>\n",
       "      <td>1700029749</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>41</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chatcmpl-8L3gc07NrWpn4pit6ENZI9kg669at</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Why did the apple go to the doctor? Because it...</td>\n",
       "      <td>1700029750</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>41</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chatcmpl-8L3gdnQ4r7PYeYyzMwK0US99ergnk</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Why did the apple go to the doctor? Because it...</td>\n",
       "      <td>1700029751</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>41</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chatcmpl-8L3gegHtJNu4R2agu2qe4fa6WJCPF</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Why did the apple go to the doctor? Because it...</td>\n",
       "      <td>1700029752</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>41</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id                   system_message  \\\n",
       "0  chatcmpl-8L3gbJgmQkv7MAp1k9g7PWMFVf2fv  You are a joke telling machine.   \n",
       "1  chatcmpl-8L3gbjIEI67yx86MuT8AxJUcvTUZl  You are a joke telling machine.   \n",
       "2  chatcmpl-8L3gc07NrWpn4pit6ENZI9kg669at  You are a joke telling machine.   \n",
       "3  chatcmpl-8L3gdnQ4r7PYeYyzMwK0US99ergnk  You are a joke telling machine.   \n",
       "4  chatcmpl-8L3gegHtJNu4R2agu2qe4fa6WJCPF  You are a joke telling machine.   \n",
       "\n",
       "                      user_message  \\\n",
       "0  Tell me something about apples.   \n",
       "1  Tell me something about apples.   \n",
       "2  Tell me something about apples.   \n",
       "3  Tell me something about apples.   \n",
       "4  Tell me something about apples.   \n",
       "\n",
       "                                    response_message  created_time  \\\n",
       "0  Why did the apple go to the doctor? Because it...    1700029749   \n",
       "1  Why did the apple go to the doctor? Because it...    1700029749   \n",
       "2  Why did the apple go to the doctor? Because it...    1700029750   \n",
       "3  Why did the apple go to the doctor? Because it...    1700029751   \n",
       "4  Why did the apple go to the doctor? Because it...    1700029752   \n",
       "\n",
       "                model  total_tokens  prompt_tokens  completion_tokens  seed  \\\n",
       "0  gpt-3.5-turbo-1106            41             24                 17  None   \n",
       "1  gpt-3.5-turbo-1106            41             24                 17  None   \n",
       "2  gpt-3.5-turbo-1106            41             24                 17  None   \n",
       "3  gpt-3.5-turbo-1106            41             24                 17  None   \n",
       "4  gpt-3.5-turbo-1106            41             24                 17  None   \n",
       "\n",
       "   temperature  top_p max_tokens  \n",
       "0          0.4    0.9       None  \n",
       "1          0.4    0.9       None  \n",
       "2          0.4    0.9       None  \n",
       "3          0.4    0.9       None  \n",
       "4          0.4    0.9       None  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiple message bundle dicts can be converted into pandas DataFrame\n",
    "# NOTE: if an API call fails, then `None` will be returned. `None` items cannot\n",
    "# be directly converted into pd.DataFrame\n",
    "responses = []\n",
    "with ProgressBar() as p:\n",
    "    for _ in p.track(range(5)):\n",
    "        response = model.create_chat_completion(\n",
    "            system_message=\"You are a joke telling machine.\",\n",
    "            user_message=\"Tell me something about apples.\",\n",
    "            output_format=\"bundle_dict\",\n",
    "            temperature=0.4,\n",
    "            seed=None,\n",
    "        )\n",
    "        responses += [response]\n",
    "\n",
    "df = pd.DataFrame(responses)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8L3gh1rg2KkLdMJ69kbXWUChXSSZW', 'system_message': 'You are a joke telling machine.', 'user_message': 'Tell me something about apples.', 'response_message': 'Sure! Did you hear about the apple who won the marathon? He took the lead because he knew how to stay core-geous!', 'created_time': 1700029755, 'model': 'gpt-3.5-turbo-1106', 'total_tokens': 52, 'prompt_tokens': 24, 'completion_tokens': 28, 'seed': None, 'temperature': 0.9, 'top_p': 0.9, 'max_tokens': None}\n"
     ]
    }
   ],
   "source": [
    "# If an API call fails, this method will automatically retry and make another API call.\n",
    "# By default it will retry 5 times.  We can change this value to 2.\n",
    "bundle_dict = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=\"bundle_dict\",\n",
    "    num_retries=2,\n",
    ")\n",
    "print(bundle_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8L3giY5jqb9h5ym4lIk5tcqSjbkt7', 'system_message': 'You are a joke telling machine.', 'user_message': 'Tell me something about apples.', 'response_message': 'Why did the apple go to therapy? Because it had too many core issues!', 'created_time': 1700029756, 'model': 'gpt-3.5-turbo-1106', 'total_tokens': 40, 'prompt_tokens': 24, 'completion_tokens': 16, 'seed': None, 'temperature': 0.9, 'top_p': 0.9, 'max_tokens': None}\n"
     ]
    }
   ],
   "source": [
    "# The `create_chat_completion` method is syntactic sugar for `chat_completion`.\n",
    "# It simply formats the message for us.\n",
    "system_message = \"You are a joke telling machine.\"\n",
    "user_message = \"Tell me something about apples.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "\n",
    "bundle_dict = model.chat_completion(\n",
    "    messages=messages,\n",
    "    output_format=\"bundle_dict\",\n",
    "    num_retries=2,\n",
    ")\n",
    "print(bundle_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Making API calls using asynchronous (non-blocking) client\n",
    "\n",
    " This enables concurrent API calls.  We can control the max concurrency.\n",
    "\n",
    " Async uses the asyncio paradigm.  We need to run an asyncio event loop to\n",
    " use these functions.\n",
    " NOTE: a jupyter notebook has an asyncio event loop running by default,\n",
    " but you need to create your own asyncio event loop in a python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8L3lE2R2kHYUr9pLLEjAi25JYdgEv', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=\"Why did the apple break up with the orange? Because it couldn't peel the connection anymore!\", role='assistant', function_call=None, tool_calls=None))], created=1700030036, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage=CompletionUsage(completion_tokens=19, prompt_tokens=24, total_tokens=43))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"You are a joke telling machine.\"\n",
    "user_message = \"Tell me something about apples.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "\n",
    "response = await model.async_chat_completion(messages=messages, num_retries=1)  # noqa: F704:\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'system', 'content': 'You are a joke telling machine.'},\n",
       "  {'role': 'user', 'content': 'Tell me something about apples.'}],\n",
       " [{'role': 'system', 'content': 'You are a joke telling machine.'},\n",
       "  {'role': 'user', 'content': 'Tell me something about apples.'}],\n",
       " [{'role': 'system', 'content': 'You are a joke telling machine.'},\n",
       "  {'role': 'user', 'content': 'Tell me something about apples.'}],\n",
       " [{'role': 'system', 'content': 'You are a joke telling machine.'},\n",
       "  {'role': 'user', 'content': 'Tell me something about apples.'}],\n",
       " [{'role': 'system', 'content': 'You are a joke telling machine.'},\n",
       "  {'role': 'user', 'content': 'Tell me something about apples.'}]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate Messages x 5 times so that we can make 5 API calls\n",
    "messages_list = [messages] * 5\n",
    "messages_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcc35ef89b64ef2a8248517cc00bf37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>system_message</th>\n",
       "      <th>user_message</th>\n",
       "      <th>response_message</th>\n",
       "      <th>created_time</th>\n",
       "      <th>model</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>seed</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>max_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chatcmpl-8L3lH6tXF0YpSS2tD7Qb61ueRNAS3</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Why did the apple go to therapy? Because it ha...</td>\n",
       "      <td>1700030039</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>None</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chatcmpl-8L3lH5r91VytJLTG90igzFjLsy8W7</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Why did the apple break up with the banana? Be...</td>\n",
       "      <td>1700030039</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>45</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>None</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chatcmpl-8L3lIibNjMiOWHj5uXh8zbwSdZeAP</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Sure! Did you hear about the apple who went to...</td>\n",
       "      <td>1700030040</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>54</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>None</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chatcmpl-8L3lIqWJnB1E30PvtHr18pNLa63AP</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Sure! Did you hear about the apple who won the...</td>\n",
       "      <td>1700030040</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>47</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>None</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chatcmpl-8L3lIGl1VGjvywbjHXiHx1xjPD01e</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>Why did the apple go to the doctor? Because it...</td>\n",
       "      <td>1700030040</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>41</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id                   system_message  \\\n",
       "0  chatcmpl-8L3lH6tXF0YpSS2tD7Qb61ueRNAS3  You are a joke telling machine.   \n",
       "1  chatcmpl-8L3lH5r91VytJLTG90igzFjLsy8W7  You are a joke telling machine.   \n",
       "2  chatcmpl-8L3lIibNjMiOWHj5uXh8zbwSdZeAP  You are a joke telling machine.   \n",
       "3  chatcmpl-8L3lIqWJnB1E30PvtHr18pNLa63AP  You are a joke telling machine.   \n",
       "4  chatcmpl-8L3lIGl1VGjvywbjHXiHx1xjPD01e  You are a joke telling machine.   \n",
       "\n",
       "                      user_message  \\\n",
       "0  Tell me something about apples.   \n",
       "1  Tell me something about apples.   \n",
       "2  Tell me something about apples.   \n",
       "3  Tell me something about apples.   \n",
       "4  Tell me something about apples.   \n",
       "\n",
       "                                    response_message  created_time  \\\n",
       "0  Why did the apple go to therapy? Because it ha...    1700030039   \n",
       "1  Why did the apple break up with the banana? Be...    1700030039   \n",
       "2  Sure! Did you hear about the apple who went to...    1700030040   \n",
       "3  Sure! Did you hear about the apple who won the...    1700030040   \n",
       "4  Why did the apple go to the doctor? Because it...    1700030040   \n",
       "\n",
       "                model  total_tokens  prompt_tokens  completion_tokens  seed  \\\n",
       "0  gpt-3.5-turbo-1106            40             24                 16  None   \n",
       "1  gpt-3.5-turbo-1106            45             24                 21  None   \n",
       "2  gpt-3.5-turbo-1106            54             24                 30  None   \n",
       "3  gpt-3.5-turbo-1106            47             24                 23  None   \n",
       "4  gpt-3.5-turbo-1106            41             24                 17  None   \n",
       "\n",
       "   temperature  top_p max_tokens  \n",
       "0          0.9    0.9       None  \n",
       "1          0.9    0.9       None  \n",
       "2          0.9    0.9       None  \n",
       "3          0.9    0.9       None  \n",
       "4          0.9    0.9       None  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Async Chat Completions, limit to 2 concurrent API calls at any given time\n",
    "responses_list = await model.async_chat_completions(  # noqa: F704\n",
    "    messages_list=messages_list,\n",
    "    num_concurrent=2,\n",
    "    num_retries=1,\n",
    "    output_format=\"bundle_dict\",\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(responses_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Example of using `Message` and `validation_callback`\n",
    "\n",
    " The `Message` wrapper allows packaging arbitrary user-defined metadata along with each message\n",
    " which is a good place to put labels, notes, etc.\n",
    "\n",
    " The `validation_callback` argument enables the user to define\n",
    " specific logic to validate the response from each API call to OpenAI\n",
    " for each message.  Passed into the callback function is the original\n",
    " `messages` and the `response`.  If the `messages` is a `Message` object,\n",
    " this will be returned in `validation_callback` for access to all metadata.\n",
    " `response` is the LLM response after being parsed and formated as specified\n",
    " in `output_format`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role': 'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1})\n",
      "In Callback. Response: Why did the apple go to the doctor? Because it wasn't peeling well!\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Why did the apple go to the doctor? Because it wasn't peeling well!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"You are a joke telling machine.\"\n",
    "user_message = \"Tell me something about apples.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "m = Message(messages=messages, metadata={\"a\": 1})\n",
    "\n",
    "\n",
    "def validation_callback_fn(messages, response) -> bool:\n",
    "    print(f\"In Callback. Messages: {messages}\")\n",
    "    print(f\"In Callback. Response: {response}\")\n",
    "    print(\"\\n\")\n",
    "    metadata = messages.metadata\n",
    "    if \"a\" in metadata:\n",
    "        return metadata[\"a\"] == 1\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Instantiate wrapper around OpenAI's API\n",
    "model = ChatModel(model=\"gpt-3.5-turbo-1106\")\n",
    "# Make ChatCompletion with...\n",
    "# - using Message wrapper and include metadata (ChatModel automatically unpacks Message.messages)\n",
    "# - parse raw OpenAI response into \"simple\" string format\n",
    "# - then call the `validation_callback_fn` that we defined.  ChatModel always passes in\n",
    "#   original messages input and parsed response as the 1st and 2nd arguments.  The\n",
    "#   `validation_callback_fn` can contain any logic, but ultimately needs to return `True` vs `False`\n",
    "#   to accept or reject the response.  If the response is rejected, ChatModel automatically retries.\n",
    "# - allow up to 1 retry.  If still fails/rejected after 1 retry, then will return `None`.\n",
    "response = model.chat_completion(\n",
    "    m,\n",
    "    output_format=\"simple\",\n",
    "    validation_callback=validation_callback_fn,\n",
    "    num_retries=1,\n",
    ")\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role': 'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1}),\n",
       " Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role': 'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1}),\n",
       " Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role': 'user', 'content': 'Tell me something about apples.'}], metadata={'b': 2})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiple concurrent async chat completions using Message\n",
    "# NOTE: we make the 3rd Message with different metadata.  This should cause\n",
    "# the `validation_callback_fn` to reject the response for only the 3rd Message in list\n",
    "# and retry only the 3rd Message.\n",
    "m_list = [m] * 2 + [Message(messages=messages, metadata={\"b\": 2})]\n",
    "m_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba44a2fa5074805a300566ba2c4999f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role':\n",
       "'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1})\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role':\n",
       "'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1})\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Callback. Response: Sure! Did you hear about the apple who went to the doctor? The doctor said, \"You're not \n",
       "looking so good, you're a little 'core'!\"\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Callback. Response: Sure! Did you hear about the apple who went to the doctor? The doctor said, \"You're not \n",
       "looking so good, you're a little 'core'!\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role':\n",
       "'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1})\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role':\n",
       "'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1})\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Callback. Response: Why did the apple stop in the middle of the road? Because it ran out of juice!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Callback. Response: Why did the apple stop in the middle of the road? Because it ran out of juice!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role':\n",
       "'user', 'content': 'Tell me something about apples.'}], metadata={'b': 2})\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role':\n",
       "'user', 'content': 'Tell me something about apples.'}], metadata={'b': 2})\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Callback. Response: Sure! Did you hear about the apple that went to a comedy club? It had everyone rolling in \n",
       "the aisles because it was such a \"crisp\" and \"juicy\" comedian!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Callback. Response: Sure! Did you hear about the apple that went to a comedy club? It had everyone rolling in \n",
       "the aisles because it was such a \"crisp\" and \"juicy\" comedian!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use Async Chat Completions, limit to 2 concurrent API calls at any given time & 1 retry\n",
    "responses_list = await model.async_chat_completions(  # noqa: F704\n",
    "    messages_list=m_list,\n",
    "    num_concurrent=2,\n",
    "    num_retries=1,\n",
    "    validation_callback=validation_callback_fn,\n",
    "    output_format=\"simple\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why did the apple stop in the middle of the road? Because it ran out of juice!',\n",
       " 'Sure! Did you hear about the apple who went to the doctor? The doctor said, \"You\\'re not looking so good, you\\'re a little \\'core\\'!\"',\n",
       " None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine responses.\n",
    "# - We should get valid responses for the first 2 responses.\n",
    "# - The 3rd response should always be `None` because the metadata cannot pass at\n",
    "#   `validation_callback_fn`\n",
    "responses_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
