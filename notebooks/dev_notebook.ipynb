{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Example in how to use ChatModel Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'automated_llm_eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mautomated_llm_eval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchat_model\u001b[39;00m \u001b[39mimport\u001b[39;00m ChatModel, Message\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mautomated_llm_eval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m ProgressBar, sidethread_event_loop_async_runner\n\u001b[1;32m      6\u001b[0m \u001b[39m# Instantiate wrapper around OpenAI's API\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'automated_llm_eval'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from automated_llm_eval.chat_model import ChatModel, Message\n",
    "from automated_llm_eval.utils import ProgressBar, sidethread_event_loop_async_runner\n",
    "\n",
    "\n",
    "# Instantiate wrapper around OpenAI's API\n",
    "model = ChatModel(model=\"gpt-3.5-turbo-1106\")\n",
    "# model = ChatModel(model=\"gpt-4-1106-preview\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatModel(sync_client=<openai.OpenAI object at 0x11ca88c90>, async_client=<openai.AsyncOpenAI object at 0x11caac150>, model='gpt-3.5-turbo-1106', temperature=0.5, top_p=0.5, max_tokens=300, n=1, seed=42)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can adjust other model settings globally for all API calls\n",
    "model2 = ChatModel(model=\"gpt-3.5-turbo-1106\", temperature=0.5, top_p=0.5, max_tokens=300, seed=42)\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatModel(sync_client=<openai.OpenAI object at 0x11ca88c90>, async_client=<openai.AsyncOpenAI object at 0x11caac150>, model='gpt-3.5-turbo-1106', temperature=0.5, top_p=0.5, max_tokens=None, n=1, seed=42)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `max_tokens = None` means no max_token limit (this is the default)\n",
    "model2 = ChatModel(model=\"gpt-3.5-turbo-1106\", temperature=0.5, top_p=0.5, max_tokens=None, seed=42)\n",
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Making API calls using synchronous (blocking) client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Did you hear about the apple who went to the doctor? It said, \"I'm feeling a little rotten.\"\n"
     ]
    }
   ],
   "source": [
    "# Make API call, get response message.\n",
    "# Note: `output_format = \"simple\"`\n",
    "response_message = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=\"simple\",\n",
    ")\n",
    "print(response_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8LIEizorIQmTB8O3C8YwAlafmnGGl', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=\"Why did the apple go to the doctor? Because it wasn't peeling well!\", role='assistant', function_call=None, tool_calls=None))], created=1700085680, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage=CompletionUsage(completion_tokens=17, prompt_tokens=24, total_tokens=41))\n"
     ]
    }
   ],
   "source": [
    "# Make API call, get original ChatCompletion object.\n",
    "# Note: `output_format = None`\n",
    "response = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=None,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bundle(id='chatcmpl-8LIEj0bm89tEdBtdqm2gHzS5Ng8kg', system_message='You are a joke telling machine.', user_message='Tell me something about apples.', metadata=None, response_message='Why did the apple stop in the middle of the road? Because it ran out of juice!', created_time=1700085681, model='gpt-3.5-turbo-1106', total_tokens=43, prompt_tokens=24, completion_tokens=19, seed=None, temperature=0.9, top_p=0.9, max_tokens=None)\n"
     ]
    }
   ],
   "source": [
    "# Make API call, get response packaged with input + metadata.\n",
    "# Note: `output_format = \"bundle\"`\n",
    "bundle = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=\"bundle\",\n",
    ")\n",
    "print(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8LIElldN2tGoBkLDl2dBdVrWWPWfJ', 'system_message': 'You are a joke telling machine.', 'user_message': 'Tell me something about apples.', 'metadata': None, 'response_message': 'Why did the apple go to therapy? Because it had too many core issues!', 'created_time': 1700085683, 'model': 'gpt-3.5-turbo-1106', 'total_tokens': 40, 'prompt_tokens': 24, 'completion_tokens': 16, 'seed': None, 'temperature': 0.9, 'top_p': 0.9, 'max_tokens': None}\n"
     ]
    }
   ],
   "source": [
    "# Make API call, get MessageBundle as a dict.\n",
    "# Note: `output_format = \"bundle_dict\"`\n",
    "bundle_dict = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=\"bundle_dict\",\n",
    ")\n",
    "print(bundle_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                              chatcmpl-8LIElldN2tGoBkLDl2dBdVrWWPWfJ\n",
       "system_message                         You are a joke telling machine.\n",
       "user_message                           Tell me something about apples.\n",
       "metadata                                                          None\n",
       "response_message     Why did the apple go to therapy? Because it ha...\n",
       "created_time                                                1700085683\n",
       "model                                               gpt-3.5-turbo-1106\n",
       "total_tokens                                                        40\n",
       "prompt_tokens                                                       24\n",
       "completion_tokens                                                   16\n",
       "seed                                                              None\n",
       "temperature                                                        0.9\n",
       "top_p                                                              0.9\n",
       "max_tokens                                                        None\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Message bundle dict can be converted into pandas Series easily\n",
    "s = pd.Series(bundle_dict)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6567c382d9f42828a1c17dee28d44b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>system_message</th>\n",
       "      <th>user_message</th>\n",
       "      <th>metadata</th>\n",
       "      <th>response_message</th>\n",
       "      <th>created_time</th>\n",
       "      <th>model</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>seed</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>max_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chatcmpl-8LIEvZuvS3EzxVgGWtDQAeSt0v7gy</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>None</td>\n",
       "      <td>Why did the apple go to the doctor? Because it...</td>\n",
       "      <td>1700085693</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>41</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chatcmpl-8LIEwQDOYw4kAjsTy6sybftU3lcJt</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>None</td>\n",
       "      <td>Why did the apple go to the doctor? Because it...</td>\n",
       "      <td>1700085694</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>41</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chatcmpl-8LIEwO36JC3epmP7r3nUKzTuJeU2d</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>None</td>\n",
       "      <td>Sure! Did you hear about the apple who went to...</td>\n",
       "      <td>1700085694</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>60</td>\n",
       "      <td>24</td>\n",
       "      <td>36</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chatcmpl-8LIExaDW5XR6XQUcQfc6HE0lFGYmb</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>None</td>\n",
       "      <td>Why did the apple go to the doctor? Because it...</td>\n",
       "      <td>1700085695</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>41</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chatcmpl-8LIEyDt4t3SfCUdkEDnBYVJLFsamz</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>None</td>\n",
       "      <td>Why did the apple go to the doctor? Because it...</td>\n",
       "      <td>1700085696</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>41</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id                   system_message  \\\n",
       "0  chatcmpl-8LIEvZuvS3EzxVgGWtDQAeSt0v7gy  You are a joke telling machine.   \n",
       "1  chatcmpl-8LIEwQDOYw4kAjsTy6sybftU3lcJt  You are a joke telling machine.   \n",
       "2  chatcmpl-8LIEwO36JC3epmP7r3nUKzTuJeU2d  You are a joke telling machine.   \n",
       "3  chatcmpl-8LIExaDW5XR6XQUcQfc6HE0lFGYmb  You are a joke telling machine.   \n",
       "4  chatcmpl-8LIEyDt4t3SfCUdkEDnBYVJLFsamz  You are a joke telling machine.   \n",
       "\n",
       "                      user_message metadata  \\\n",
       "0  Tell me something about apples.     None   \n",
       "1  Tell me something about apples.     None   \n",
       "2  Tell me something about apples.     None   \n",
       "3  Tell me something about apples.     None   \n",
       "4  Tell me something about apples.     None   \n",
       "\n",
       "                                    response_message  created_time  \\\n",
       "0  Why did the apple go to the doctor? Because it...    1700085693   \n",
       "1  Why did the apple go to the doctor? Because it...    1700085694   \n",
       "2  Sure! Did you hear about the apple who went to...    1700085694   \n",
       "3  Why did the apple go to the doctor? Because it...    1700085695   \n",
       "4  Why did the apple go to the doctor? Because it...    1700085696   \n",
       "\n",
       "                model  total_tokens  prompt_tokens  completion_tokens  seed  \\\n",
       "0  gpt-3.5-turbo-1106            41             24                 17  None   \n",
       "1  gpt-3.5-turbo-1106            41             24                 17  None   \n",
       "2  gpt-3.5-turbo-1106            60             24                 36  None   \n",
       "3  gpt-3.5-turbo-1106            41             24                 17  None   \n",
       "4  gpt-3.5-turbo-1106            41             24                 17  None   \n",
       "\n",
       "   temperature  top_p max_tokens  \n",
       "0          0.4    0.9       None  \n",
       "1          0.4    0.9       None  \n",
       "2          0.4    0.9       None  \n",
       "3          0.4    0.9       None  \n",
       "4          0.4    0.9       None  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiple message bundle dicts can be converted into pandas DataFrame\n",
    "# NOTE: if an API call fails, then `None` will be returned. `None` items cannot\n",
    "# be directly converted into pd.DataFrame\n",
    "responses = []\n",
    "with ProgressBar() as p:\n",
    "    for _ in p.track(range(5)):\n",
    "        response = model.create_chat_completion(\n",
    "            system_message=\"You are a joke telling machine.\",\n",
    "            user_message=\"Tell me something about apples.\",\n",
    "            output_format=\"bundle_dict\",\n",
    "            temperature=0.4,\n",
    "            seed=None,\n",
    "        )\n",
    "        responses += [response]\n",
    "\n",
    "df = pd.DataFrame(responses)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8LIEzFIdVzBSsJJ21hng3AZI8PRLc', 'system_message': 'You are a joke telling machine.', 'user_message': 'Tell me something about apples.', 'metadata': None, 'response_message': \"Sure, here's a joke about apples:\\n\\nWhy did the apple stop in the middle of the road?\\n\\nBecause it ran out of juice!\", 'created_time': 1700085697, 'model': 'gpt-3.5-turbo-1106', 'total_tokens': 52, 'prompt_tokens': 24, 'completion_tokens': 28, 'seed': None, 'temperature': 0.9, 'top_p': 0.9, 'max_tokens': None}\n"
     ]
    }
   ],
   "source": [
    "# If an API call fails, this method will automatically retry and make another API call.\n",
    "# By default it will retry 5 times.  We can change this value to 2.\n",
    "bundle_dict = model.create_chat_completion(\n",
    "    system_message=\"You are a joke telling machine.\",\n",
    "    user_message=\"Tell me something about apples.\",\n",
    "    output_format=\"bundle_dict\",\n",
    "    num_retries=2,\n",
    ")\n",
    "print(bundle_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8LIF1fVbUbsko55IgU36Eb1EstOiz', 'system_message': 'You are a joke telling machine.', 'user_message': 'Tell me something about apples.', 'metadata': None, 'response_message': \"Why did the apple go to the doctor? Because it wasn't peeling well!\", 'created_time': 1700085699, 'model': 'gpt-3.5-turbo-1106', 'total_tokens': 41, 'prompt_tokens': 24, 'completion_tokens': 17, 'seed': None, 'temperature': 0.9, 'top_p': 0.9, 'max_tokens': None}\n"
     ]
    }
   ],
   "source": [
    "# The `create_chat_completion` method is syntactic sugar for `chat_completion`.\n",
    "# It simply formats the message for us.\n",
    "system_message = \"You are a joke telling machine.\"\n",
    "user_message = \"Tell me something about apples.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "\n",
    "bundle_dict = model.chat_completion(\n",
    "    messages=messages,\n",
    "    output_format=\"bundle_dict\",\n",
    "    num_retries=2,\n",
    ")\n",
    "print(bundle_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Making API calls using asynchronous (non-blocking) client\n",
    "\n",
    " This enables concurrent API calls.  We can control the max concurrency.\n",
    "\n",
    " Async uses the asyncio paradigm.  We need to run an asyncio event loop to\n",
    " use these functions.\n",
    " NOTE: a jupyter notebook has an asyncio event loop running by default,\n",
    " but you need to create your own asyncio event loop in a python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8LIL18DbBB8s5U3msQG6anMD6DQPJ', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Why did the apple stop in the middle of the road?\\n\\nBecause it ran out of juice!', role='assistant', function_call=None, tool_calls=None))], created=1700086071, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage=CompletionUsage(completion_tokens=19, prompt_tokens=24, total_tokens=43))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"You are a joke telling machine.\"\n",
    "user_message = \"Tell me something about apples.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "\n",
    "response = await model.async_chat_completion(messages=messages, num_retries=1)  # noqa: F704:\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'system', 'content': 'You are a joke telling machine.'},\n",
       "  {'role': 'user', 'content': 'Tell me something about apples.'}],\n",
       " [{'role': 'system', 'content': 'You are a joke telling machine.'},\n",
       "  {'role': 'user', 'content': 'Tell me something about apples.'}],\n",
       " [{'role': 'system', 'content': 'You are a joke telling machine.'},\n",
       "  {'role': 'user', 'content': 'Tell me something about apples.'}],\n",
       " [{'role': 'system', 'content': 'You are a joke telling machine.'},\n",
       "  {'role': 'user', 'content': 'Tell me something about apples.'}],\n",
       " [{'role': 'system', 'content': 'You are a joke telling machine.'},\n",
       "  {'role': 'user', 'content': 'Tell me something about apples.'}]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate Messages x 5 times so that we can make 5 API calls\n",
    "messages_list = [messages] * 5\n",
    "messages_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2511123356b4e30a9303e4e8624e0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>system_message</th>\n",
       "      <th>user_message</th>\n",
       "      <th>metadata</th>\n",
       "      <th>response_message</th>\n",
       "      <th>created_time</th>\n",
       "      <th>model</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>seed</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>max_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chatcmpl-8LIL3Z0WuNEHO6x6A3vjEU5qHYEya</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>None</td>\n",
       "      <td>Sure! Did you hear about the apple that joined...</td>\n",
       "      <td>1700086073</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>47</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>None</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chatcmpl-8LIL3kuZ296LHjrzvV2k250r1VjkX</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>None</td>\n",
       "      <td>Sure! Did you hear about the apple that went o...</td>\n",
       "      <td>1700086073</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>48</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>None</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chatcmpl-8LIL4ERAgZT72ybOqtqsyqSr1DC6s</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>None</td>\n",
       "      <td>Sure! Did you hear about the apple who went to...</td>\n",
       "      <td>1700086074</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>68</td>\n",
       "      <td>24</td>\n",
       "      <td>44</td>\n",
       "      <td>None</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chatcmpl-8LIL4q1BuM549bQSQ1hxc3p0kPXVE</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>None</td>\n",
       "      <td>Why did the apple go to the doctor? Because it...</td>\n",
       "      <td>1700086074</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>41</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chatcmpl-8LIL48ILhKaXSpRBpM94aav2uC551</td>\n",
       "      <td>You are a joke telling machine.</td>\n",
       "      <td>Tell me something about apples.</td>\n",
       "      <td>None</td>\n",
       "      <td>Sure! Did you hear about the apple that went o...</td>\n",
       "      <td>1700086074</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>53</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>None</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id                   system_message  \\\n",
       "0  chatcmpl-8LIL3Z0WuNEHO6x6A3vjEU5qHYEya  You are a joke telling machine.   \n",
       "1  chatcmpl-8LIL3kuZ296LHjrzvV2k250r1VjkX  You are a joke telling machine.   \n",
       "2  chatcmpl-8LIL4ERAgZT72ybOqtqsyqSr1DC6s  You are a joke telling machine.   \n",
       "3  chatcmpl-8LIL4q1BuM549bQSQ1hxc3p0kPXVE  You are a joke telling machine.   \n",
       "4  chatcmpl-8LIL48ILhKaXSpRBpM94aav2uC551  You are a joke telling machine.   \n",
       "\n",
       "                      user_message metadata  \\\n",
       "0  Tell me something about apples.     None   \n",
       "1  Tell me something about apples.     None   \n",
       "2  Tell me something about apples.     None   \n",
       "3  Tell me something about apples.     None   \n",
       "4  Tell me something about apples.     None   \n",
       "\n",
       "                                    response_message  created_time  \\\n",
       "0  Sure! Did you hear about the apple that joined...    1700086073   \n",
       "1  Sure! Did you hear about the apple that went o...    1700086073   \n",
       "2  Sure! Did you hear about the apple who went to...    1700086074   \n",
       "3  Why did the apple go to the doctor? Because it...    1700086074   \n",
       "4  Sure! Did you hear about the apple that went o...    1700086074   \n",
       "\n",
       "                model  total_tokens  prompt_tokens  completion_tokens  seed  \\\n",
       "0  gpt-3.5-turbo-1106            47             24                 23  None   \n",
       "1  gpt-3.5-turbo-1106            48             24                 24  None   \n",
       "2  gpt-3.5-turbo-1106            68             24                 44  None   \n",
       "3  gpt-3.5-turbo-1106            41             24                 17  None   \n",
       "4  gpt-3.5-turbo-1106            53             24                 29  None   \n",
       "\n",
       "   temperature  top_p max_tokens  \n",
       "0          0.9    0.9       None  \n",
       "1          0.9    0.9       None  \n",
       "2          0.9    0.9       None  \n",
       "3          0.9    0.9       None  \n",
       "4          0.9    0.9       None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Async Chat Completions, limit to 2 concurrent API calls at any given time\n",
    "responses_list = await model.async_chat_completions(  # noqa: F704\n",
    "    messages_list=messages_list,\n",
    "    num_concurrent=2,\n",
    "    num_retries=1,\n",
    "    output_format=\"bundle_dict\",\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(responses_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Example of using `Message` and `validation_callback`\n",
    "\n",
    " The `Message` wrapper allows packaging arbitrary user-defined metadata along with each message\n",
    " which is a good place to put labels, notes, etc.\n",
    "\n",
    " The `validation_callback` argument enables the user to define\n",
    " specific logic to validate the response from each API call to OpenAI\n",
    " for each message.  Passed into the callback function is the original\n",
    " `messages` and the `response`.  If the `messages` is a `Message` object,\n",
    " this will be returned in `validation_callback` for access to all metadata.\n",
    " `response` is the LLM response after being parsed and formated as specified\n",
    " in `output_format`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role': 'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1})\n",
      "In Callback. Response: {'id': 'chatcmpl-8LILBkfH7yk4FZO1Q8EStTOU2xQMI', 'system_message': 'You are a joke telling machine.', 'user_message': 'Tell me something about apples.', 'metadata': {'a': 1}, 'response_message': 'Why did the apple go to school?\\n\\nBecause it wanted to be a \"smart\" apple!', 'created_time': 1700086081, 'model': 'gpt-3.5-turbo-1106', 'total_tokens': 43, 'prompt_tokens': 24, 'completion_tokens': 19, 'seed': None, 'temperature': 0.9, 'top_p': 0.9, 'max_tokens': None}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-8LILBkfH7yk4FZO1Q8EStTOU2xQMI',\n",
       " 'system_message': 'You are a joke telling machine.',\n",
       " 'user_message': 'Tell me something about apples.',\n",
       " 'metadata': {'a': 1},\n",
       " 'response_message': 'Why did the apple go to school?\\n\\nBecause it wanted to be a \"smart\" apple!',\n",
       " 'created_time': 1700086081,\n",
       " 'model': 'gpt-3.5-turbo-1106',\n",
       " 'total_tokens': 43,\n",
       " 'prompt_tokens': 24,\n",
       " 'completion_tokens': 19,\n",
       " 'seed': None,\n",
       " 'temperature': 0.9,\n",
       " 'top_p': 0.9,\n",
       " 'max_tokens': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"You are a joke telling machine.\"\n",
    "user_message = \"Tell me something about apples.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "m = Message(messages=messages, metadata={\"a\": 1})\n",
    "\n",
    "\n",
    "def validation_callback_fn(messages, response) -> bool:\n",
    "    print(f\"In Callback. Messages: {messages}\")\n",
    "    print(f\"In Callback. Response: {response}\")\n",
    "    print(\"\\n\")\n",
    "    metadata = messages.metadata\n",
    "    if \"a\" in metadata:\n",
    "        return metadata[\"a\"] == 1\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Instantiate wrapper around OpenAI's API\n",
    "model = ChatModel(model=\"gpt-3.5-turbo-1106\")\n",
    "# Make ChatCompletion with...\n",
    "# - using Message wrapper and include metadata (ChatModel automatically unpacks Message.messages)\n",
    "# - parse raw OpenAI response into \"simple\" string format\n",
    "# - then call the `validation_callback_fn` that we defined.  ChatModel always passes in\n",
    "#   original messages input and parsed response as the 1st and 2nd arguments.  The\n",
    "#   `validation_callback_fn` can contain any logic, but ultimately needs to return `True` vs `False`\n",
    "#   to accept or reject the response.  If the response is rejected, ChatModel automatically retries.\n",
    "# - allow up to 1 retry.  If still fails/rejected after 1 retry, then will return `None`.\n",
    "response = model.chat_completion(\n",
    "    m,\n",
    "    output_format=\"bundle_dict\",\n",
    "    validation_callback=validation_callback_fn,\n",
    "    num_retries=1,\n",
    ")\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role': 'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1}),\n",
       " Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role': 'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1}),\n",
       " Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role': 'user', 'content': 'Tell me something about apples.'}], metadata={'b': 2})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiple concurrent async chat completions using Message\n",
    "# NOTE: we make the 3rd Message with different metadata.  This should cause\n",
    "# the `validation_callback_fn` to reject the response for only the 3rd Message in list\n",
    "# and retry only the 3rd Message.\n",
    "msg_list = [m] * 2 + [Message(messages=messages, metadata={\"b\": 2})]\n",
    "msg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc9ac47809d4fff8acc9db24a0e3650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role':\n",
       "'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1})\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role':\n",
       "'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1})\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Callback. Response: {'id': 'chatcmpl-8LILIcDJyzGrKomK2TVnDzWwooqkX', 'system_message': 'You are a joke telling \n",
       "machine.', 'user_message': 'Tell me something about apples.', 'metadata': {'a': 1}, 'response_message': 'Why did \n",
       "the apple go to therapy?\\n\\nBecause it had too many core issues!', 'created_time': 1700086088, 'model': \n",
       "'gpt-3.5-turbo-1106', 'total_tokens': 40, 'prompt_tokens': 24, 'completion_tokens': 16, 'seed': None, \n",
       "'temperature': 0.9, 'top_p': 0.9, 'max_tokens': None}\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Callback. Response: {'id': 'chatcmpl-8LILIcDJyzGrKomK2TVnDzWwooqkX', 'system_message': 'You are a joke telling \n",
       "machine.', 'user_message': 'Tell me something about apples.', 'metadata': {'a': 1}, 'response_message': 'Why did \n",
       "the apple go to therapy?\\n\\nBecause it had too many core issues!', 'created_time': 1700086088, 'model': \n",
       "'gpt-3.5-turbo-1106', 'total_tokens': 40, 'prompt_tokens': 24, 'completion_tokens': 16, 'seed': None, \n",
       "'temperature': 0.9, 'top_p': 0.9, 'max_tokens': None}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role':\n",
       "'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1})\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role':\n",
       "'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1})\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Callback. Response: {'id': 'chatcmpl-8LILIcbSRuWmhzdoe9PSICAtjv80c', 'system_message': 'You are a joke telling \n",
       "machine.', 'user_message': 'Tell me something about apples.', 'metadata': {'a': 1}, 'response_message': \"Why did \n",
       "the apple break up with the orange? Because it couldn't handle the pithy comments!\", 'created_time': 1700086088, \n",
       "'model': 'gpt-3.5-turbo-1106', 'total_tokens': 45, 'prompt_tokens': 24, 'completion_tokens': 21, 'seed': None, \n",
       "'temperature': 0.9, 'top_p': 0.9, 'max_tokens': None}\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Callback. Response: {'id': 'chatcmpl-8LILIcbSRuWmhzdoe9PSICAtjv80c', 'system_message': 'You are a joke telling \n",
       "machine.', 'user_message': 'Tell me something about apples.', 'metadata': {'a': 1}, 'response_message': \"Why did \n",
       "the apple break up with the orange? Because it couldn't handle the pithy comments!\", 'created_time': 1700086088, \n",
       "'model': 'gpt-3.5-turbo-1106', 'total_tokens': 45, 'prompt_tokens': 24, 'completion_tokens': 21, 'seed': None, \n",
       "'temperature': 0.9, 'top_p': 0.9, 'max_tokens': None}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role':\n",
       "'user', 'content': 'Tell me something about apples.'}], metadata={'b': 2})\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Callback. Messages: Message(messages=[{'role': 'system', 'content': 'You are a joke telling machine.'}, {'role':\n",
       "'user', 'content': 'Tell me something about apples.'}], metadata={'b': 2})\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Callback. Response: {'id': 'chatcmpl-8LILI6BdPHkmvkyH6cGSJM1X8YkrW', 'system_message': 'You are a joke telling \n",
       "machine.', 'user_message': 'Tell me something about apples.', 'metadata': {'b': 2}, 'response_message': \"Why did \n",
       "the apple go to the doctor? Because it wasn't peeling well!\", 'created_time': 1700086088, 'model': \n",
       "'gpt-3.5-turbo-1106', 'total_tokens': 41, 'prompt_tokens': 24, 'completion_tokens': 17, 'seed': None, \n",
       "'temperature': 0.9, 'top_p': 0.9, 'max_tokens': None}\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Callback. Response: {'id': 'chatcmpl-8LILI6BdPHkmvkyH6cGSJM1X8YkrW', 'system_message': 'You are a joke telling \n",
       "machine.', 'user_message': 'Tell me something about apples.', 'metadata': {'b': 2}, 'response_message': \"Why did \n",
       "the apple go to the doctor? Because it wasn't peeling well!\", 'created_time': 1700086088, 'model': \n",
       "'gpt-3.5-turbo-1106', 'total_tokens': 41, 'prompt_tokens': 24, 'completion_tokens': 17, 'seed': None, \n",
       "'temperature': 0.9, 'top_p': 0.9, 'max_tokens': None}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use Async Chat Completions, limit to 2 concurrent API calls at any given time & 1 retry\n",
    "responses_list = await model.async_chat_completions(  # noqa: F704\n",
    "    messages_list=msg_list,\n",
    "    num_concurrent=2,\n",
    "    num_retries=1,\n",
    "    validation_callback=validation_callback_fn,\n",
    "    output_format=\"bundle_dict\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'chatcmpl-8LILIcbSRuWmhzdoe9PSICAtjv80c',\n",
       "  'system_message': 'You are a joke telling machine.',\n",
       "  'user_message': 'Tell me something about apples.',\n",
       "  'metadata': {'a': 1},\n",
       "  'response_message': \"Why did the apple break up with the orange? Because it couldn't handle the pithy comments!\",\n",
       "  'created_time': 1700086088,\n",
       "  'model': 'gpt-3.5-turbo-1106',\n",
       "  'total_tokens': 45,\n",
       "  'prompt_tokens': 24,\n",
       "  'completion_tokens': 21,\n",
       "  'seed': None,\n",
       "  'temperature': 0.9,\n",
       "  'top_p': 0.9,\n",
       "  'max_tokens': None},\n",
       " {'id': 'chatcmpl-8LILIcDJyzGrKomK2TVnDzWwooqkX',\n",
       "  'system_message': 'You are a joke telling machine.',\n",
       "  'user_message': 'Tell me something about apples.',\n",
       "  'metadata': {'a': 1},\n",
       "  'response_message': 'Why did the apple go to therapy?\\n\\nBecause it had too many core issues!',\n",
       "  'created_time': 1700086088,\n",
       "  'model': 'gpt-3.5-turbo-1106',\n",
       "  'total_tokens': 40,\n",
       "  'prompt_tokens': 24,\n",
       "  'completion_tokens': 16,\n",
       "  'seed': None,\n",
       "  'temperature': 0.9,\n",
       "  'top_p': 0.9,\n",
       "  'max_tokens': None},\n",
       " None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine responses.\n",
    "# - We should get valid responses for the first 2 responses.\n",
    "# - The 3rd response should always be `None` because the metadata cannot pass at\n",
    "#   `validation_callback_fn`\n",
    "responses_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Calling Async function from Sync code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatModel(model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "system_message = \"You are a joke telling machine.\"\n",
    "user_message = \"Tell me something about apples.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "m = Message(messages=messages, metadata={\"a\": 1})\n",
    "msg_list = [m] * 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc97460112941dab45a8111848605a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/philipchung/Developer/automated-llm-eval/automated_llm_eval/chat_model.py:328: UserWarning: Failed to create\n",
       "ChatCompletion with arguments: dict_items([('messages', Message(messages=[{'role': 'system', 'content': 'You are a \n",
       "joke telling machine.'}, {'role': 'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1})), \n",
       "('model', 'gpt-3.5-turbo-1106'), ('temperature', 0.9), ('top_p', 0.9), ('max_tokens', None), ('n', 1), ('seed', \n",
       "None)])\n",
       "Exception: \n",
       "Retries left: 5\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/philipchung/Developer/automated-llm-eval/automated_llm_eval/chat_model.py:328: UserWarning: Failed to create\n",
       "ChatCompletion with arguments: dict_items([('messages', Message(messages=[{'role': 'system', 'content': 'You are a \n",
       "joke telling machine.'}, {'role': 'user', 'content': 'Tell me something about apples.'}], metadata={'a': 1})), \n",
       "('model', 'gpt-3.5-turbo-1106'), ('temperature', 0.9), ('top_p', 0.9), ('max_tokens', None), ('n', 1), ('seed', \n",
       "None)])\n",
       "Exception: \n",
       "Retries left: 5\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Bundle(id='chatcmpl-8LILXRTjxlnIu0lk6yvEvZZHNGynL', system_message='You are a joke telling machine.', user_message='Tell me something about apples.', metadata={'a': 1}, response_message='Why did the apple go to school?\\n\\nBecause it wanted to be a \"smart\" apple!', created_time=1700086103, model='gpt-3.5-turbo-1106', total_tokens=43, prompt_tokens=24, completion_tokens=19, seed=None, temperature=0.9, top_p=0.9, max_tokens=None),\n",
       " Bundle(id='chatcmpl-8LILSe9zlppqjSFLLppyJKvx9PK4q', system_message='You are a joke telling machine.', user_message='Tell me something about apples.', metadata={'a': 1}, response_message=\"Sure! Did you hear about the apple who went to a party? He was the apple of everyone's eye!\", created_time=1700086098, model='gpt-3.5-turbo-1106', total_tokens=47, prompt_tokens=24, completion_tokens=23, seed=None, temperature=0.9, top_p=0.9, max_tokens=None),\n",
       " Bundle(id='chatcmpl-8LILTqQPgygaqYYw4qKBFzwIQNERc', system_message='You are a joke telling machine.', user_message='Tell me something about apples.', metadata={'a': 1}, response_message='Why did the apple go to school? Because it wanted to be a \"smart\" apple!', created_time=1700086099, model='gpt-3.5-turbo-1106', total_tokens=43, prompt_tokens=24, completion_tokens=19, seed=None, temperature=0.9, top_p=0.9, max_tokens=None)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Up until now, we have used `await` to call async functions and wait for their completion.\n",
    "# However, `await` this can only be used within async functions.\n",
    "# we are not allowed to call `await` from a function not defined with `async def`\n",
    "responses = await model.async_chat_completions(\n",
    "    messages_list=msg_list, num_concurrent=2, output_format=\"bundle\"\n",
    ")\n",
    "responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d1827308174e9ca73b9cdac88a256d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Bundle(id='chatcmpl-8LILaCN5zBKFT2f1NVO8KVfdWOHKF', system_message='You are a joke telling machine.', user_message='Tell me something about apples.', metadata={'a': 1}, response_message='Sure! Did you hear about the apple who went to school? He wanted to be a \"smart\" apple!', created_time=1700086106, model='gpt-3.5-turbo-1106', total_tokens=47, prompt_tokens=24, completion_tokens=23, seed=None, temperature=0.9, top_p=0.9, max_tokens=None),\n",
       " Bundle(id='chatcmpl-8LILZ5oC2cegD0t6n7tAGYDB8dgwI', system_message='You are a joke telling machine.', user_message='Tell me something about apples.', metadata={'a': 1}, response_message=\"Why did the apple go to the doctor? Because it wasn't peeling well!\", created_time=1700086105, model='gpt-3.5-turbo-1106', total_tokens=41, prompt_tokens=24, completion_tokens=17, seed=None, temperature=0.9, top_p=0.9, max_tokens=None),\n",
       " Bundle(id='chatcmpl-8LILZXMugSlaP2R2JGWiWF0tKmK4J', system_message='You are a joke telling machine.', user_message='Tell me something about apples.', metadata={'a': 1}, response_message=\"Why did the apple break up with the orange? Because it couldn't find the core of the relationship!\", created_time=1700086105, model='gpt-3.5-turbo-1106', total_tokens=45, prompt_tokens=24, completion_tokens=21, seed=None, temperature=0.9, top_p=0.9, max_tokens=None)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have created a helper function to address this issue.\n",
    "#\n",
    "# Call async method from sync function without using `await` keyword.\n",
    "# This involves creating an event loop on another thread, then\n",
    "# waiting for result on main thread and shutting down the event loop on other thread.\n",
    "\n",
    "result = sidethread_event_loop_async_runner(\n",
    "    async_function=model.async_chat_completions(\n",
    "        messages_list=msg_list, num_concurrent=2, output_format=\"bundle\"\n",
    "    )\n",
    ")\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
